# This is a basic workflow to help you get started with Actions

name: Algolia Scraper

# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the master branch
on:
  schedule:
    - cron: '0 9 * * WED'
  push:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
    # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
    - uses: actions/checkout@v2

    # Runs a single command using the runners shell
    - name: Clone the scraper repository
      run: git clone https://github.com/algolia/docsearch-scraper scraper-repository

    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.6.11
    
    - name: Install pipenv
      run: sudo apt install pipenv
      working-directory: ./scraper-repository
    
    - name: Install python dependencies
      run: pipenv install
      working-directory: ./scraper-repository

    - name: Enter pipenv shell
      run: pipenv install
      working-directory: ./scraper-repository

    - name: Create environment file
      run: |
        'echo "APPLICATION_ID=$APP_ID" > .env'
        'echo "APPLICATION_ID=$API_KEY" > .env'
      env:
        APP_ID: ${{ secrets.ALGOLIA_APP_ID }}
        API_KEY: ${{ secrets.ALGOLIA_ADMIN_API_KEY }}
      working-directory: ./scraper-repository
     
    - name: Build the docker container
      run: ./docsearch docker:build
      working-directory: ./scraper-repository

    - name: Scrape the documentation
      run: ./docsearch docker:run config.json
      working-directory: ./scraper-repository
